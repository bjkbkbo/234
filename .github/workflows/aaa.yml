name: a1a

on:
  # 每次该 yml 文件被 push 修改后自动运行
  push:
    paths:
      - ".github/workflows/aaa.yml"

  # 允许在 GitHub Actions 页面手动启动
  workflow_dispatch:

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 360

    env:
      # frpc 配置文件下载地址（参考 minecraft-frpc.yml）
      FRPC_INI_URL: "https://nrt.kmoljklj.top/frpc/frpc.ini"
      FRP_VERSION: "0.55.1"

      # 要运行的 Ollama 模型（按你的要求固定 latest）
      OLLAMA_MODEL: "gpt-oss:latest"

      # 将 Ollama 的数据/模型/临时目录迁移到 /mnt（该分区通常更大）
      # 说明：
      # - OLLAMA_MODELS：模型存放目录
      # - OLLAMA_HOME：Ollama 数据目录（用于避免 ~/.ollama 占满 /dev/root）
      # - OLLAMA_TMPDIR：拉取/解压过程的临时目录（避免临时文件占满根分区）
      OLLAMA_MODELS: /mnt/ollama/models
      OLLAMA_HOME: /mnt/ollama
      OLLAMA_TMPDIR: /mnt/ollama/tmp

    steps:

      - name: Free disk space (remove preinstalled toolchains)
        shell: bash
        run: |
          set -euxo pipefail

          # These directories are typically huge on ubuntu-latest runners.
          # Remove what we don't need to make room for large Ollama models.
          sudo rm -rf /usr/share/dotnet || true
          sudo rm -rf /usr/local/lib/android || true
          sudo rm -rf /opt/ghc || true
          sudo rm -rf /usr/local/share/powershell || true
          sudo rm -rf /usr/local/.ghcup || true

          # Clean package caches
          sudo apt-get clean || true
          sudo rm -rf /var/lib/apt/lists/* || true

          # If docker is present and has images, prune them (safe for this workflow)
          docker system prune -af || true

          df -h

      - name: Prepare Ollama dirs (on /mnt)
        shell: bash
        run: |
          set -euxo pipefail

          # Create dirs on /mnt
          sudo mkdir -p "$OLLAMA_MODELS" "$OLLAMA_TMPDIR"
          sudo chown -R "$USER":"$USER" /mnt/ollama

          # Also make ~/.ollama point to /mnt/ollama to prevent any fallback from filling /dev/root
          rm -rf "$HOME/.ollama" || true
          ln -s /mnt/ollama "$HOME/.ollama"

          ls -ld /mnt/ollama /mnt/ollama/models /mnt/ollama/tmp "$HOME/.ollama"
          df -h /mnt || true

      - name: Install dependencies
        shell: bash
        run: |
          set -euxo pipefail
          sudo apt-get update
          sudo apt-get install -y curl ca-certificates jq

      - name: Download frpc (frp linux_amd64)
        shell: bash
        run: |
          set -euxo pipefail

          FRP_TGZ="frp_${FRP_VERSION}_linux_amd64.tar.gz"
          FRP_URL="https://github.com/fatedier/frp/releases/download/v${FRP_VERSION}/${FRP_TGZ}"

          curl -fL --retry 3 --retry-all-errors -o "$FRP_TGZ" "$FRP_URL"
          tar -xzf "$FRP_TGZ"

          install -m 0755 "frp_${FRP_VERSION}_linux_amd64/frpc" "./frpc"

      - name: Fetch frpc.ini (placeholder supported)
        shell: bash
        run: |
          set -euxo pipefail

          if curl -fsSL --retry 3 --retry-all-errors -o frpc.ini "$FRPC_INI_URL"; then
            echo "Downloaded frpc.ini from: $FRPC_INI_URL"
          else
            echo "WARNING: Failed to download frpc.ini from: $FRPC_INI_URL"
            echo "Creating a placeholder frpc.ini so the workflow can proceed."

            # Avoid heredoc indentation issues in GitHub Actions/YAML by using printf.
            printf '%s\n' \
              '[common]' \
              'server_addr = 127.0.0.1' \
              'server_port = 7000' \
              '' \
              '# Example placeholder' \
              '[example]' \
              'type = tcp' \
              'local_ip = 127.0.0.1' \
              'local_port = 11434' \
              'remote_port = 11434' \
              > frpc.ini
          fi

          echo "---- frpc.ini ----"
          sed -n '1,200p' frpc.ini

      - name: Start frpc in background (nohup)
        shell: bash
        run: |
          set -euxo pipefail

          nohup ./frpc -c ./frpc.ini > frpc.log 2>&1 &

          # Basic liveness check
          sleep 2
          pgrep -fa "./frpc" || (echo "frpc failed to start"; echo "---- frpc.log ----"; tail -n 200 frpc.log; exit 1)

          echo "frpc started."
          echo "---- frpc.log (tail) ----"
          tail -n 50 frpc.log || true

      - name: Install Ollama
        shell: bash
        run: |
          set -euxo pipefail
          # Official install script
          curl -fsSL https://ollama.com/install.sh | sudo -E sh

          # Show version for debugging
          ollama --version

      - name: Start Ollama server in background
        shell: bash
        run: |
          set -euxo pipefail

          # Start ollama server
          nohup ollama serve > ollama.log 2>&1 &

          # Wait until the API is reachable
          for i in {1..60}; do
            if curl -fsS http://127.0.0.1:11434/api/tags > /dev/null; then
              echo "Ollama is up."
              break
            fi
            sleep 1
          done

          curl -fsS http://127.0.0.1:11434/api/tags | jq '.' || true
          echo "---- ollama.log (tail) ----"
          tail -n 100 ollama.log || true

      - name: Pull model (gpt-oss:latest)
        shell: bash
        run: |
          set -euxo pipefail
          ollama pull "$OLLAMA_MODEL"

      - name: Run model in background (keep session online)
        shell: bash
        run: |
          set -euxo pipefail

          # `ollama run` is interactive. Running it without stdin keeps it waiting for user input,
          # which is what we want to keep the job "online".
          nohup bash -lc "exec ollama run '$OLLAMA_MODEL'" > ollama-run.log 2>&1 &

          # Basic liveness check
          sleep 2
          pgrep -fa "ollama run" || (echo "ollama run failed to start"; echo "---- ollama-run.log ----"; tail -n 200 ollama-run.log; exit 1)

          echo "ollama run started in background."
          echo "---- ollama-run.log (tail) ----"
          tail -n 50 ollama-run.log || true

      - name: Keep workflow alive (until timeout)
        shell: bash
        run: |
          set -euxo pipefail
          echo "Keeping CI online. This job will stay running until timeout-minutes is reached, or until manually cancelled."

          # Keep the job alive; also periodically show last logs to help debugging.
          while true; do
            sleep 60
            if ! pgrep -fa "ollama serve" >/dev/null; then
              echo "ollama serve is not running anymore; dumping logs and exiting."
              tail -n 200 ollama.log || true
              exit 1
            fi
            if ! pgrep -fa "ollama run" >/dev/null; then
              echo "ollama run is not running anymore; dumping logs and exiting."
              tail -n 200 ollama-run.log || true
              exit 1
            fi
            echo "---- heartbeat $(date -u +%F_%T) ----"
            tail -n 20 frpc.log || true
            tail -n 20 ollama.log || true
            tail -n 20 ollama-run.log || true
          done
